.PHONY: build up down restart mcp test ollama-setup ollama-pull

build:
	@echo "ğŸ”¨ Building Next.js application..."
	@cd next-connect-ui && npm install && npm run build
	@echo "âœ… Next.js build completed!"
	@echo ""
	@echo "ğŸ”¨ Building Docker images..."
	@docker compose build
	@echo "âœ… Docker build completed successfully!"
	@echo "ğŸ“Œ Run 'make up' to start the server"

up:
	@echo "ğŸš€ Starting LangConnect server..."
	@docker compose up -d
	@echo "âœ… Server started successfully!"
	@echo "ğŸ“Œ Access points:"
	@echo "   - API Server: http://localhost:8080"
	@echo "   - API Docs: http://localhost:8080/docs"
	@echo "   - Next.js UI: http://localhost:3000"
	@echo "   - PostgreSQL: localhost:5432"

down:
	@echo "ğŸ›‘ Stopping LangConnect server..."
	@docker compose down
	@echo "âœ… Server stopped successfully!"

restart:
	@echo "ğŸ”„ Restarting LangConnect server..."
	@docker-compose down
	@docker-compose up -d
	@echo "âœ… Server restarted successfully!"

mcp:
	@echo "ğŸ”§ Creating MCP configuration..."
	@uv run python mcpserver/create_mcp_json.py
	@echo "âœ… MCP configuration created successfully!"

TEST_FILE ?= tests/unit_tests

test:
	./run_tests.sh $(TEST_FILE)

ollama-setup:
	@echo "ğŸ¤– Setting up Ollama embedding model..."
	@docker compose up -d ollama
	@echo "â³ Waiting for Ollama to start..."
	@sleep 10
	@echo "ğŸ“¥ Pulling nomic-embed-text model..."
	@docker exec langconnect-ollama ollama pull nomic-embed-text
	@echo "âœ… Ollama setup completed!"
	@echo "ğŸ“Œ You can now set USE_OLLAMA_EMBEDDINGS=true in your .env file"
	@echo "ğŸ“Œ Ollama is running on port 11435 (to avoid conflict with local Ollama on 11434)"

ollama-pull:
	@echo "ğŸ“¥ Pulling Ollama embedding model..."
	@docker exec langconnect-ollama ollama pull nomic-embed-text
	@echo "âœ… Model pulled successfully!"
	@echo "ğŸ“Œ Ollama is running on port 11435 (external)"

